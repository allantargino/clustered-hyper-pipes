{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperPipes\n",
    "\n",
    "HyperPipes é um algoritmo de classificação desenvolvido por Lucio de Souza Coelho,  Universidade Federal de Minas Gerais, e posteriormente aperfeiçoado por Len Trigg, The University of Waikato, originalmente publicado na versão estável 3.2 do Weka em 1999. O algoritmo básico consiste em criar hipercubos definidos a partir das fronteiras de cada dimensão das classes presentes, em novas instâncias são classificadas de acordo com o hipercubo que mais engloba-la. Apesar de ser um algoritmo muito simples, possui como maior vantagem ser extremamente rápido, devido a sua baixa complexidade de tempo e memória computacional. Ele funciona bem quando há uma quantidade muito grande de atributos e/ou as classes são bem separadas e definidas no espaço amostral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação\n",
    "\n",
    "O algoritmo é implementado abaixo. Separou-se o conceito de um único *HiperPipe* (representando uma classe), do conceito de *n* multiplos *HiperPipes*, para facilitar a compreensão da implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\altargin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperPipe:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.n_dimensions = 0\n",
    "        self.numerical_bounds = []\n",
    "\n",
    "    def fit(self, data_x, target_class):\n",
    "        self.target_class = target_class\n",
    "        self.n_dimensions = data_x.shape[1]\n",
    "\n",
    "        # Initializes bounds\n",
    "        for i in range(self.n_dimensions):\n",
    "            bounds = []\n",
    "            bounds.append(float('+inf'))  # lower bound\n",
    "            bounds.append(float('-inf'))  # upper bound\n",
    "            self.numerical_bounds.append(bounds)\n",
    "\n",
    "        # Add instances\n",
    "        for i in range(data_x.shape[0]):\n",
    "            self.__add_instance__(data_x[i])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __add_instance__(self, data_x):\n",
    "        #check boundaries\n",
    "        for i in range(self.n_dimensions):\n",
    "            if(data_x[i] < self.numerical_bounds[i][0]):\n",
    "                self.numerical_bounds[i][0] = data_x[i]\n",
    "            if(data_x[i] > self.numerical_bounds[i][1]):\n",
    "                self.numerical_bounds[i][1] = data_x[i]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def partial_contains(self, data_x):\n",
    "        count = 0\n",
    "        for i in range(self.n_dimensions):\n",
    "            if(data_x[i] > self.numerical_bounds[i][0] and data_x[i] < self.numerical_bounds[i][1]):\n",
    "                count += 1\n",
    "        score = float(count) / self.n_dimensions\n",
    "\n",
    "        return (score, self.target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class HyperPipes(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.hyper_pipes = []\n",
    "\n",
    "    def fit(self, data_x, data_y):\n",
    "        self.y_unique_values, self.y_unique_indices = np.unique(\n",
    "            data_y, return_inverse=True)\n",
    "        self.n_y_unique = self.y_unique_values.shape[0]\n",
    "        self.hyper_pipes = [HyperPipe() for i in range(self.n_y_unique)]\n",
    "\n",
    "        for i in range(self.n_y_unique):\n",
    "            target_class = self.y_unique_values[i]\n",
    "            target_class_indices = np.where(data_y == target_class)\n",
    "            data_x_filtered = data_x[target_class_indices]\n",
    "            self.hyper_pipes[i].fit(data_x_filtered, target_class)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, data_x):\n",
    "        predictions = []\n",
    "        for instance in data_x:\n",
    "            partial_results = []\n",
    "            for i in range(self.n_y_unique):\n",
    "                partial_results.append(self.hyper_pipes[i].partial_contains(instance))\n",
    "            best = max(partial_results,key=lambda item:item[0])[1]\n",
    "            predictions.append(best)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "O HyperPipes consegue ser performático em bases de dados com excessivos atributos e/ou classes bem separadas no espaço amostral, porém geralmente não desempenha bem caso contrário. Quando o algoritmo foi incorporado no pacote de Machine Learning Weka, muitos estudos de benchmark de classificadores para diversos problemas foram realizados.\n",
    "\n",
    "Delgado e Amorim realizaram um estudo de avaliação envolvendo 179 classificadores oriundos de 17 famílias (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classiﬁers, boosting, bagging, stacking, random forests e outros ensembles, generalized linear models, nearest-neighbors, partial least squares e principal component regression, logistic e multinomial regression, multiple adaptive regression splines e outros métodos), implementados em Weka, R, C e Matlab. Eles usaram 121 bases de dados distintas, representando a maior parte dos presentes na UCI e de outras fontes. Segundo os rankings de Friedman, acurácia média e Cohen k, o HyperPipes ocupou a posição 167 dos 179 classificadores das diversas famílias. Em sua família específica, a de outros métodos, ocupou a posição 8 dentre os 10 presentes.\n",
    "\n",
    "Em outro estudo de para um problema específico de uso de aprendizado de máquina aplicado a trajetórias dinâmicas de moléculas, HyperPipes obteve o 44 lugar dos 65 classificadores de diversas famílias do Weka, ao passo que foi o primeiro dentre os 3 de categoria outro (ou miscelânea).\n",
    "\n",
    "Abaixo é feito uma simples demonstração de performance do HiperPipes frente a outros algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(score):\n",
    "    mean = np.mean(score)\n",
    "    std  = np.std(score)\n",
    "    print('* ' + name + ':\\t' + str(mean) + ' ± ' + str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_names = [\"HyperPipes\", \"Naive Bayes\"]\n",
    "\n",
    "classifiers = [\n",
    "    HyperPipes(),\n",
    "    GaussianNB()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = [\"Iris\", \"Wine\"]\n",
    "datasets = [\n",
    "    load_iris(),\n",
    "    load_wine()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris\n",
      "* HyperPipes:\t0.9466666666666667 ± 0.06531972647421808\n",
      "* Naive Bayes:\t0.9533333333333334 ± 0.04268749491621898\n",
      "Wine\n",
      "* HyperPipes:\t0.8953216374269004 ± 0.07571688098076965\n",
      "* Naive Bayes:\t0.9616959064327485 ± 0.042442001415448946\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in zip(datasets_names, datasets):\n",
    "\n",
    "    X, y = dataset.data, dataset.target\n",
    "\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    folds = StratifiedKFold(y, n_folds=10, random_state=8893)\n",
    "    \n",
    "    print(dataset_name)\n",
    "    for name, clf in zip(classifiers_names, classifiers):\n",
    "        score = cross_val_score(clf, X, y, scoring='accuracy', cv=folds)\n",
    "        print_score(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposição de Classes\n",
    "\n",
    "Pelo fato do HyperPipes ser um classificador extremamente simples e estar associado a separação quase que linear das classes, ele acaba performando muito mal em testes de benchmarks com bases de dados de características genéricas. \n",
    "\n",
    "Uma maneira de aumentar a performance do algoritmo seria aplicar algum tipo de pré-processamento a entrada dos dados a fim de melhorar os modelos produzidos pelo algoritmo. Estudos propõem a utilização de técnicas de decomposição de classes usando algoritmos de clustering com o objetivo de melhorar a acurácia de classificadores simples – uma vez que a separação de classes prévia torne a nova distribuição mais fácil de ser explorada pelo classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação Uniforme\n",
    "\n",
    "Abaixo é implementado um agente de decomposição de classes que recebe como argumento de seu construtor um algoritmo de clusterização qualquer, bem como um parametro *k* do número de clusters por classe. Como pode ser visto na implementação, este decompositor opera de maneira **uniforme** sobre todas as classes, usando o mesmo parâmetro *k* de clusters para todas classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformClassDecomposition:\n",
    "    \n",
    "    def __init__(self, algorithm, k):\n",
    "        algorithm.set_params(n_clusters=k)\n",
    "        self.algorithm = algorithm\n",
    "        self.k = k\n",
    "\n",
    "    def decompose(self, data_x, data_y):\n",
    "        self.y_unique_values, self.y_unique_indices = np.unique(data_y, return_inverse=True)\n",
    "        self.n_y_unique = self.y_unique_values.shape[0]\n",
    "\n",
    "        renamed_data_x = np.array([])\n",
    "        renamed_data_y = np.array([])\n",
    "        for i in range(self.n_y_unique):\n",
    "            target_class = self.y_unique_values[i]\n",
    "            target_class_indices = np.where(data_y == target_class)\n",
    "\n",
    "            data_i_x = data_x[target_class_indices]\n",
    "            data_i_y = data_y[target_class_indices]\n",
    "            fitted = self.algorithm.fit(data_i_x)\n",
    "            labels = fitted.labels_\n",
    "            \n",
    "            for j in range(self.k):\n",
    "                indices_j = np.where(labels == j)\n",
    "                data_j_x = data_i_x[indices_j]\n",
    "                data_j_y = data_i_y[indices_j]\n",
    "\n",
    "                relabed = self.relabel(data_j_y, j)\n",
    "\n",
    "                renamed_data_x = np.append(renamed_data_x, data_j_x)\n",
    "                renamed_data_y = np.append(renamed_data_y, relabed)\n",
    "\n",
    "        X = np.reshape(renamed_data_x, (data_x.shape[0], data_x.shape[1]))\n",
    "        y = renamed_data_y\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def relabel(self, data, cluster_idx):\n",
    "        return np.array(list(map(lambda item: str(item) + chr(65 + cluster_idx), data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo\n",
    "\n",
    "Um exemplo da ação de decomposição é mostrado abaixo.\n",
    "\n",
    "Originalmente, o dataset de *Wine* possuia 3 classes, com respectivos labels de **0**, **1** e **2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_wine()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a decomposição de classes uniform, com *k=2*, teremos 6 novas classes.\n",
    "\n",
    "Cada nova classe será resultado de um cluster, cujo nome é obtido a partir da função *relabel*.\n",
    "Na implementação acima, o nome original da classe é concatenado com uma caractere, iniciando a indexação em *A*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0A', '0B', '1A', '1B', '2A', '2B'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 2\n",
    "\n",
    "kmeans = KMeans(random_state=42)\n",
    "cd = UniformClassDecomposition(kmeans, k)\n",
    "\n",
    "X_cd, y_cd = cd.decompose(X,y)\n",
    "\n",
    "set(y_cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterers_names = [\"K-Means\", \"Spectral Clustering\", \"Agglomerative Clustering\"]\n",
    "\n",
    "clusterers = [\n",
    "    KMeans(random_state=8893),\n",
    "    SpectralClustering(random_state=8893),\n",
    "    AgglomerativeClustering(n_clusters=k)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "K-Means\n",
      "Iris\n",
      "* HyperPipes:\t0.9466666666666667 ± 0.06531972647421808\n",
      "* Naive Bayes:\t0.9533333333333334 ± 0.04268749491621898\n",
      "Wine\n",
      "* HyperPipes:\t0.8953216374269004 ± 0.07571688098076965\n",
      "* Naive Bayes:\t0.9616959064327485 ± 0.042442001415448946\n",
      "\n",
      "Spectral Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.9466666666666667 ± 0.06531972647421808\n",
      "* Naive Bayes:\t0.9533333333333334 ± 0.04268749491621898\n",
      "Wine\n",
      "* HyperPipes:\t0.8953216374269004 ± 0.07571688098076965\n",
      "* Naive Bayes:\t0.9616959064327485 ± 0.042442001415448946\n",
      "\n",
      "Agglomerative Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.9466666666666667 ± 0.06531972647421808\n",
      "* Naive Bayes:\t0.9533333333333334 ± 0.04268749491621898\n",
      "Wine\n",
      "* HyperPipes:\t0.8953216374269004 ± 0.07571688098076965\n",
      "* Naive Bayes:\t0.9616959064327485 ± 0.042442001415448946\n",
      "\n",
      "2\n",
      "K-Means\n",
      "Iris\n",
      "* HyperPipes:\t0.8786939775910364 ± 0.10255373198181233\n",
      "* Naive Bayes:\t0.9204647884794943 ± 0.06969287195477494\n",
      "Wine\n",
      "* HyperPipes:\t0.7750148655953609 ± 0.08744553306306571\n",
      "* Naive Bayes:\t0.8428053589856995 ± 0.06080521400389247\n",
      "\n",
      "Spectral Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.8776785714285715 ± 0.07680141394401176\n",
      "* Naive Bayes:\t0.9266865079365079 ± 0.06623307422705929\n",
      "Wine\n",
      "* HyperPipes:\t0.868860877684407 ± 0.11678101680016828\n",
      "* Naive Bayes:\t0.9401493930905696 ± 0.08541396647649664\n",
      "\n",
      "Agglomerative Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.8762896825396826 ± 0.07271094508408905\n",
      "* Naive Bayes:\t0.9262896825396825 ± 0.05411076611403894\n",
      "Wine\n",
      "* HyperPipes:\t0.7409756621947025 ± 0.10324312162758398\n",
      "* Naive Bayes:\t0.8504334365325077 ± 0.07285717551678962\n",
      "\n",
      "3\n",
      "K-Means\n",
      "Iris\n",
      "* HyperPipes:\t0.7508656256024676 ± 0.08648923882819698\n",
      "* Naive Bayes:\t0.9256641604010024 ± 0.06309761614063472\n",
      "Wine\n",
      "* HyperPipes:\t0.642456090091539 ± 0.1292642650098716\n",
      "* Naive Bayes:\t0.8542919073530527 ± 0.09274184316138984\n",
      "\n",
      "Spectral Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.7326166643813703 ± 0.12675189812833892\n",
      "* Naive Bayes:\t0.9192824822236588 ± 0.05999408145727692\n",
      "Wine\n",
      "* HyperPipes:\t0.8394444444444444 ± 0.1339269194489533\n",
      "* Naive Bayes:\t0.9238888888888889 ± 0.11357816691600546\n",
      "\n",
      "Agglomerative Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.7757713854772678 ± 0.12446100565466656\n",
      "* Naive Bayes:\t0.9104384830855418 ± 0.07796454671664917\n",
      "Wine\n",
      "* HyperPipes:\t0.6613706112429023 ± 0.12121255109438339\n",
      "* Naive Bayes:\t0.8193366828390047 ± 0.09696994639380463\n",
      "\n",
      "4\n",
      "K-Means\n",
      "Iris\n",
      "* HyperPipes:\t0.7667946196390469 ± 0.06981080327311824\n",
      "* Naive Bayes:\t0.9200932813316716 ± 0.07863398870199477\n",
      "Wine\n",
      "* HyperPipes:\t0.5725 ± 0.13213845385675707\n",
      "* Naive Bayes:\t0.9012698412698412 ± 0.0813559424233958\n",
      "\n",
      "Spectral Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.6950877192982455 ± 0.11709259920865828\n",
      "* Naive Bayes:\t0.8115664160401004 ± 0.07861029527819542\n",
      "Wine\n",
      "* HyperPipes:\t0.8317020697167756 ± 0.15971689133796996\n",
      "* Naive Bayes:\t0.9067592592592593 ± 0.14499068722754593\n",
      "\n",
      "Agglomerative Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.7346848181058708 ± 0.08442233117702766\n",
      "* Naive Bayes:\t0.905593149540518 ± 0.0796346090524372\n",
      "Wine\n",
      "* HyperPipes:\t0.5725412985707103 ± 0.1202533661259765\n",
      "* Naive Bayes:\t0.8243367090425915 ± 0.09254756372913552\n",
      "\n",
      "5\n",
      "K-Means\n",
      "Iris\n",
      "* HyperPipes:\t0.6196567400943831 ± 0.12795171213541587\n",
      "* Naive Bayes:\t0.8644703208690622 ± 0.09664635487595577\n",
      "Wine\n",
      "* HyperPipes:\t0.4563558972286594 ± 0.08713639702229507\n",
      "* Naive Bayes:\t0.8291062293300145 ± 0.12880888415379824\n",
      "\n",
      "Spectral Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.564033446773385 ± 0.13146651822161917\n",
      "* Naive Bayes:\t0.8409488207011426 ± 0.08539578905819718\n",
      "Wine\n",
      "* HyperPipes:\t0.7976806239737274 ± 0.18647436905306705\n",
      "* Naive Bayes:\t0.8823706896551725 ± 0.17578016331815718\n",
      "\n",
      "Agglomerative Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.6065263661459314 ± 0.0769979399599324\n",
      "* Naive Bayes:\t0.9032011104837192 ± 0.08257608646950604\n",
      "Wine\n",
      "* HyperPipes:\t0.44955190234602005 ± 0.1341746170012753\n",
      "* Naive Bayes:\t0.8467536385183443 ± 0.11794817254318629\n",
      "\n",
      "6\n",
      "K-Means\n",
      "Iris\n",
      "* HyperPipes:\t0.649818720637135 ± 0.1245189386531222\n",
      "* Naive Bayes:\t0.8749017947099788 ± 0.09950521110906321\n",
      "Wine\n",
      "* HyperPipes:\t0.3780380730380731 ± 0.12452839495253289\n",
      "* Naive Bayes:\t0.8105050505050505 ± 0.06689456458253516\n",
      "\n",
      "Spectral Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.5547286041682999 ± 0.12976430581542736\n",
      "* Naive Bayes:\t0.8269887782608203 ± 0.10835145449816155\n",
      "Wine\n",
      "* HyperPipes:\t0.7852688172043011 ± 0.19363441935006154\n",
      "* Naive Bayes:\t0.8782795698924731 ± 0.19815124249788857\n",
      "\n",
      "Agglomerative Clustering\n",
      "Iris\n",
      "* HyperPipes:\t0.6163319575161681 ± 0.12748735333670527\n",
      "* Naive Bayes:\t0.8771000490737333 ± 0.08075120787280284\n",
      "Wine\n",
      "* HyperPipes:\t0.443082874240769 ± 0.1503365883725609\n",
      "* Naive Bayes:\t0.8174445281813704 ± 0.13321747843134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in k_values:\n",
    "    print(k)\n",
    "\n",
    "    for cst_name, cst in zip(clusterers_names, clusterers):\n",
    "        print(cst_name)\n",
    "\n",
    "        cd = UniformClassDecomposition(cst, k)\n",
    "\n",
    "        for ds_name, ds in zip(datasets_names, datasets):\n",
    "            print(ds_name)\n",
    "\n",
    "            X, y = ds.data, ds.target\n",
    "\n",
    "            X_cd, y_cd = cd.decompose(X,y)\n",
    "\n",
    "            X_cd = StandardScaler().fit_transform(X_cd)\n",
    "\n",
    "            folds = StratifiedKFold(y_cd, n_folds=10, random_state=8893)\n",
    "\n",
    "            for name, clf in zip(classifiers_names, classifiers):\n",
    "                score = cross_val_score(clf, X_cd, y_cd, scoring='accuracy', cv=folds)\n",
    "                print_score(score)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
